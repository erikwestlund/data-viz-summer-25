---
title: "Data Preparation Example"
author: "Erik Westlund"
date: "2025-06-11"
date-modified: "`r format(Sys.Date(), '%Y-%m-%d')`"
---

## Data Source

We use [CDC's PRAMStat data for 2011](https://data.cdc.gov/Maternal-Child-Health/CDC-PRAMStat-Data-for-2011/ese6-rqpq/about_data).

## Note

In this file, I use exposition to explain what I am doing and what I am thinking. In this context, this is an unknown data set we're working with and I am trying to teach others.

For a project orreport, you would want to first [read the data file documentation and codebook](https://www.cdc.gov/prams/php/questionnaires/index.html) first to understand the data.  Your commentary in the notebook should be targeted to the audience, conveying everything required for people to understand the steps you took and why you took them.

This data is aggregated at the location level and comes to us in "long" format.

## Loading Required Libraries

We first need to load in the libraries we'll use. 

We will use:

* `readr` to load in the data ( I prefer `readr` over the built-in `read.csv` function because it is faster and has better default settings for avoiding common issues with CSVs.)
* `dplyr` to clean data.  
* `kableExtra` is used to make tables look nice. 
* `DT` to make tables interactive and get nice pagination on long tables.
* `tidyr` to pivot the data to wide format.

```{r setup}
#| message: false

library(dplyr)
library(DT)
library(kableExtra)
library(readr)
library(tidyr)
```

To hide this code from the output, I typically prepend the above code with.

```r
#| echo: false
#| message: false
```

Here, for illustration, I hide messages but not the code block.

## Read and Explore the Data

Here we read the data using the `readr::read_csv` function. Note that we use `here::here` to ensure the path to the data is correct relative to the project root.

We then use the `glimpse` function to take a look at the data. Glimpse tells us the number of rows and columns in the data, the names of the columns, the type of each column, and the first few rows of the data.

I create a new object `df_clean` to store the cleaned data. For now, we'll just copy the original data to this object. This is a good habit to get into, as it allows you to keep the original data for reference and to easily revert to it if needed.

```{r load-data}
#| message: false

# Load the data using here::here to ensure correct path resolution
df <- read_csv(here::here("data", "raw", "cdc_PRAMStat_Data_for_2011_20250610.csv"))

# Take a look at the data structure
df |> glimpse()

df_clean <- df

```

## Tabbing Out Data

Always read the codebook first to understand the data. Nevertheless, it's still helpful to tab out the data to get a sense of the variables and their values.

Let's first look at the topics availble

```{r tab-data}
#| message: false

table(df$Topic) |> kable()
```

Suppose we're intersted in the relationship between Alcohol Use and Mental Health.

Let's filter the data to include only those topics, and then look at the questions available.

```{r filter-data-topics}
#| message: false

df |>
  filter(Topic %in% c("Alcohol Use", "Mental Health")) |>
  select(QuestionId, Topic, Question) |>
  distinct() |>
  kable()
```

## Filtering Data

Suppose we're interested in the relationship between drinking in the first three months before pregnancy and reporting depression and anxiety. H

```{r filter-data}
#| message: false

question_ids_of_interest <- c(
  "QUO271", # Binge drinking in the first three months before pregnancy
  "QUO8",   # Any alcohol use in the first three months before pregnancy
  "QUO133", # Depression reported in the first three months before pregnancy
  "QUO232"  # Anxiety reported in the first three months before pregnancy
)

df_clean <- df_clean |>
  filter(QuestionId %in% question_ids_of_interest)

df_clean |>
  head() |>
  kable()
```

I personally prefer to remove values from the data set we do not need. We can always add them back in. By using select we also reorder the columns to make it easier to take in.

```{r remove-unneeded-columns}

df_clean <- df_clean |>
  select(
    QuestionId,
    Topic,
    Question,
    Response,
    Data_Value,
    Data_Value_Unit,
    Data_Value_Type,
    LocationAbbr,
    LocationDesc,
    Break_Out,
    Break_Out_Category
  )
```

## Renaming Variables

I prefer to rename variables to follow a consistent style and naming convention (e.g., lowercase, snake_case, noun first).  Here I also rename variables to be more descriptive, such as with subgroups. Some would prefer keeping the original names, but I prefer the ergonomics of this approach.

We can use the `DT::datatable` function to view the data in our document in an interactive, filterable way. You'll want to be careful with inline this data, but with public data, this is a good way to share data (or a selection of it) with others.

```{r rename-variables}
#| message: false

df_clean <- df_clean |>
  rename_with(tolower) |>
  rename(
    question_id = questionid,
    value = data_value,
    unit = data_value_unit,
    type = data_value_type,
    location_abbr = locationabbr,
    location = locationdesc,
    subgroup = break_out,
    subgroup_cat = break_out_category
  )

DT::datatable(df_clean)
```

## Understanding The Data

First note that this data still needs work. Even without the codebook, we can seee:

* The response/value has "yes," "no", and `NA` values.
* The `location_abbr` has both aggregations and location values. 
* The meaning of each row depends on the `subgroup` and `subgroup_cat` variables.

First, let's get a sense of the locations in the data

```{r locations}
#| message: false
df_clean |> 
  group_by(location) |>
  summarise(
    location_abbr = first(location_abbr),
    n = n()
  ) |>
kable()
```

We see that some non-standard choices are made here: New York and New York City are both listed as locations. Also, aggregations are included under PRAMS Total.

Lets get an overview of the subgroup categories:

```{r subgroups}
#| message: false

df_clean |> 
  group_by(subgroup_cat, subgroup) |>
  summarise(n = n()) |>
  kable()

```

We now have a sense of the structure of the entire data file.

Let's finally use the `group_by` and `summarise` functions to try narrow down groups to Ns of 1.

```{r summarize-data}
#| message: false

df_summary <- df_clean |>
  group_by(question_id, location_abbr, subgroup_cat, subgroup, response) |>
  summarise(
    n = n(),
    question = first(question),
    mean_value = mean(value)
  )

DT::datatable(df_summary)
```

We now know that the following data combine to form a "base" row:

* question_id
* location_abbr
* subgroup_cat
* subgroup
* response

With this in mind, we can now see that the following data combine to form a "base" row.

## Preparing The Data For Visualizaiton

You'll notice how the data is in a kind of `long` format: there are yes, no, and NA values for each question. We want to get all values per question per location/subgroup/subgroup_cat.

Steps:

* Filter out the aggregate (non-location specific) values
* Filter out NAs on the subroup (break out) category: of course, you would need to identify WHY these are NAs and address this issue if there is a reason.

We can then use the `tidyr` package to pivot the data to wide format.

```{r final-data-processing} 
df_final <- df_clean |>
  filter(location_abbr != "PRAMS Total") |>
  filter(!is.na(subgroup)) |>
  filter(response == "YES") |>  # Only keep YES responses
  pivot_wider(
    id_cols = c(location_abbr, subgroup_cat, subgroup),
    names_from = question_id,
    values_from = value,
    names_prefix = "q_"  # Add prefix to question ID columns
  ) |>
  rename(
    depression_within_3_months_birth = q_QUO133,
    anxiety_within_3_months_birth = q_QUO232,
    alcohol_use_within_3_months_birth = q_QUO8,
    binge_drinking_within_3_months_birth = q_QUO271
  ) |> 
  arrange(location_abbr, subgroup_cat, subgroup)
  

DT::datatable(df_final)
```

## Data Diagnostics

Let's do a quick analysis of missing values.

```{r missing}

df_final |> 
  summarise(
    across(everything(), ~ 100 * sum(is.na(.)) / n())
  ) |>
  kable()

```
Let's examine missing data across all measures, by location.

```{r missing-summary}
df_final |>
  group_by(location_abbr) |>
  summarise(
    depression_missing = 100 * mean(is.na(depression_within_3_months_birth)),
    anxiety_missing = 100 * mean(is.na(anxiety_within_3_months_birth)),
    alcohol_missing = 100 * mean(is.na(alcohol_use_within_3_months_birth)),
    binge_missing = 100 * mean(is.na(binge_drinking_within_3_months_birth))
  ) |>
  arrange(location_abbr) |>
  kable(digits = 1)
```

Data on alcohol use is more comprehensive than on anxiety and depression.  Please note that just because we're focused on data visualization does not mean our concerns over missing data are any less important than, say, when we are doing staitstical modeling.

## Save The Data

Now, let's save this processed data to an RDS file. Before we do, let's merge back in our full location for reference and turn our location variables into factors.

```{r save-data}
#| message: false

df_final <- df_final |>
  left_join(
    df_clean |>
      select(location_abbr, location) |>
      distinct(),
    by = "location_abbr"
  ) |>
  mutate(
    location_abbr = factor(location_abbr),
    location = factor(location)
  )

saveRDS(df_final, here::here("data", "processed", "cdc_prams_df_final.rds"))
```



